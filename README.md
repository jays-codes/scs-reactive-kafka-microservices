# scs-reactive-kafka-microservices
Jay's project/practice repo for Event-driven Microservices using Reactive Kafka and Spring Cloud Stream

#### proj scs-kafka-sandbox (jayaslabs.kafka; SpringBoot 3.5.5, jdk 21; Cloud Stream, Spring for Apache Kafka, Lombok, spring-cloud-stream-binder-kafka-reactive)

- section9: created app.yaml for tuple implementation - added bindings definition for processor-out-0, processor-out-1; added test pkg/class, FanOutTupleTest
- section9: [BP] implement advanced Fan-Out using Tuple2 pattern without StreamBridge for explicit multiple output streams; Created FanOutProcessor.java demonstrating reactive stream splitting with compile-time type safety using Function<Flux<Message<OrderEvent>>, Tuple2<Flux<DigitalDelivery>, Flux<PhysicalDelivery>>> signature; Implements sophisticated stream sharing via Sinks.Many<OrderEvent> multicast sink feeding two independent output streams: digital deliveries (all orders) and physical deliveries (filtered PHYSICAL orders only); Uses Tuples.of() for explicit output contract, transform() operators for stream processing, and filter() for conditional routing; Added comprehensive section9 package structure mirroring section8 but with pure reactive approach; Configured application-section9.yaml for Tuple2-based topology; Updated application.yaml to section9 profile; Message flow: OrderEvent → shared Sinks.Many → parallel stream processing → Tuple2<Flux<Digital>, Flux<Physical>> → Spring Cloud Stream binding → independent topic routing; Demonstrates enterprise pattern for type-safe multi-output processors, stream analytics systems, and scenarios requiring explicit compile-time output contracts with full reactive stream control
- section8: Added FanOutProcessorMessageBuilder to show implementation of Fanout using MessageBuilder without using StreamBridge and using Tuples
- section8: [BP] implement Fan-Out messaging pattern with content-based routing and advanced functional composition; Created FanOutProcessor.java demonstrating one-to-many message distribution where physical orders trigger fan-out to BOTH digital-delivery-topic AND physical-delivery-topic while digital orders route only to digital-delivery-topic; Implements elegant functional composition using Consumer<OrderEvent> chains with digitalSend.andThen(physicalSend) for fan-out behavior; Added comprehensive section8 package structure: producer.OrderEventProducerConfig (reactive message generation), processor.FanOutProcessor (core routing logic), consumer.DigitalDeliveryConsumer/PhysicalDeliveryConsumer (downstream processors), dto package (OrderEvent/OrderType/DigitalDelivery/PhysicalDelivery records), config.DeliveryChannelProperties (topic constants); Created FanOutTest.java integration test validating fan-out behavior: digital orders → 1 digital delivery, physical orders → 1 digital + 1 physical delivery using Sinks-based test producers and StepVerifier assertions; Configured application-section8.yaml with StreamBridge topology for fan-out scenarios; Updated application.yaml to section8 profile; Message flow: OrderEvent → FanOutProcessor router → digitalSend/fanOut functions → StreamBridge.send() → multiple topics → independent consumers; Demonstrates enterprise fan-out pattern for notification systems, order processing, and multi-channel delivery scenarios
- Accompanying Test for CharFinder below; uses 3 Sinks.Many<String/Char/String> reqSink to emit strings to string-topic, charSink to consume from char-topic, dltSink to consume for dlt-topic
- [BP] wrote a tool, CharFinder, to route a message to Dead-Letter-Topic (DLT) on exception side-effect. Exception side effect is handled via reactive error recovery via .onErrorResume() and Mono.fromRunnable(), which calls handleError() - which writes the message to DLT
- same as previous commit but this time used implicit binding (direct topic)
- section6.*; modified OrderRouter to dynamically route without using StreamBridge, and instead using Message header ("spring.cloud.stream.sendto.destination")
- create + fix section5: resolve OrderRouterTest timeout issues and Spring profile isolation problems; Fixed critical Spring Cloud Function definition bug where commas were incorrectly used instead of semicolons as function separators (orderProcessor,testProducer,testDDConsumer,testPDConsumer → orderProcessor;testProducer;testDDConsumer;testPDConsumer), causing Spring to treat all functions as single invalid function name resulting in no function registration and broken message flow; Added explicit spring.profiles.active=section5 in @TestPropertySource to ensure proper component scanning isolation preventing test interference from other packages; Enhanced AbstractIntegrationTest.java with reactive Kafka testing utilities: added createSender() and toSenderRecord() helper methods for KafkaSender creation and SenderRecord building; Created OrderRouterTest.java with complete integration test using Sinks-based producer/consumer pattern for controlled message emission and StepVerifier for reactive stream testing; Implemented inner @TestConfiguration class with testProducer (Supplier<Flux<OrderEvent>>), testDDConsumer and testPDConsumer (Consumer<Flux<DigitalDelivery/PhysicalDelivery>>) beans for isolated test environment; Test validates complete message routing pipeline: OrderEvent production → orderProcessor consumption → StreamBridge routing → delivery-specific topic distribution → consumer verification; Demonstrates proper Spring Cloud Stream test isolation, function definition syntax, and reactive integration testing patterns; Message flow: testProducer → order-events-topic → orderProcessor → StreamBridge(digital/physical-delivery-out) → delivery topics → test consumers → Sinks capture → StepVerifier assertions
- Content Based Dynamic Routing: Use topic instead of explicit bindings; modified yaml to remove explicit bindings; modified OrderRouter to pass topic to streamBridge.send() instead of binding (explicit)
- modified OrderRouter: removed enum based properties, created utility class  DeliveryChannelProperties instead; created explicit bindings (no cloud function defined) in yaml - digital-delivery-out, physical-delivery-out; modified OrderRouter to use the explicit bindings
- refactor section5: created processor.OrderRouter; implement enum-based channel routing with type safety; Added DeliveryChannel.java enum with OrderType → channel mapping and findByOrderType() lookup method; Modified OrderRouter.java to use enum-based routing eliminating hardcoded channel strings; Added application-section5.yaml with complete function definitions (orderEventProducer;orderProcessor;digitalDeliveryConsumer;physicalDeliveryConsumer) and binding configurations; Implemented type-safe content-based routing pattern using DeliveryChannel.findByOrderType() for compile-time safety; Enhanced sendToChannel() method with generic error handling and consistent logging; Upgraded pom.xml Spring Boot parent 3.5.4 → 3.5.5; Message flow: OrderEvent → DeliveryChannel enum lookup → type-safe StreamBridge.send() → delivery-specific consumers; Demonstrates enterprise pattern replacing magic strings with enums for maintainable, extensible, and type-safe channel management
- section5; created consumer.Digital/PhysicalDeliverConsumer with SC Function physical/digitalDeliveryConsumer():Function<Flux<Message<Physical/DigitalDelivery>>, Mono<Void>>; and printMsgDetails(CustomRecord<Physical/DigitalDelevery>); probably can be refactored to have BaseDeliveryConsumer
- section5: created producer.OrderEventProducerConfig (@Configuration) with Stream Cloud Function orderEventProducer returning Supplier<Flux<Message<OrderEvent>>> (@Bean) (Reactive producer with MessageBuilder pattern), and toMessage() to convert int series in flux to Message<OrderEvent>
- created section5 pkgs: consumer, producer, processor, dto. In section5.dto, created java records to represent messages consumed and produced by processor: dto.OrderEvent (with enum OrderType), dto.DigitalDelivery (DD), dto.PhysicalDelivery (PD). Processor consumes OrderEvent (OE) and produces DD or PD depending on OrderType
- started section5 (new pkg) to demo Content Based Message rounting using StreamBridge.

- refactor section4: upgrade KafkaProducerTest to industry best practices with DTO/Mapper pattern testing; Modified KafkaProducerTestConfiguration.java to use ConcurrentLinkedQueue<CustomRecord<String>> instead of Message<String> for domain-level testing, added MessageConverter::toRecord transformation in testConsumer() bean for Message<String> → CustomRecord<String> mapping; Updated KafkaProducerTest.java to test key-value pairs via testKafkaProducerWithKeyValue() method using CustomRecord assertions for record.message() and record.key() validation, replaced array-based verification with stream().toList() for type safety; Modified application.yaml to use section4 profile; Demonstrates enterprise testing pattern separating Spring Cloud Stream framework concerns (Message<T>) from business domain objects (CustomRecord<T>); Message flow: KafkaProducer → Message<String> with keys → testConsumer → MessageConverter.toRecord() → CustomRecord<String> → ConcurrentLinkedQueue → test verification with payload/key assertions; Enables clean architecture testing with thread-safe capture, proper test isolation, and domain-focused validation while maintaining external configuration separation for better maintainability and reusability across test scenarios
- demonstrate section2: Spring Cloud Stream automatic message wrapping capability; Modified KafkaConsumer.java consumer() bean from Consumer<Flux<String>> to Consumer<Flux<Message<String>>> while keeping KafkaProducer.java unchanged with Supplier<Flux<String>> (simple String payload); Added import org.springframework.messaging.Message; Updated application.yaml to use section2 profile; Demonstrates framework's intelligent auto-wrapping where simple String producers can be consumed as Message<String> objects; Producer sends: "message-4" (String) → Framework auto-wraps → Consumer receives: Message<String> with full metadata (payload, headers, partitionId, offset, acknowledgment, but kafka_receivedMessageKey=null since no key set); Message flow: Flux<String> → SCS auto-wrapping → Message<String> creation → Kafka topic → Message<String> consumer → access to acknowledgment/headers; Shows asymmetric producer-consumer pattern enabling backward compatibility, gradual migration paths, and consumer-side acknowledgment capabilities regardless of producer complexity; Validates that simple payloads work seamlessly with Message-aware consumers for maximum framework flexibility
- enhanced section4: implement manual message acknowledgment with DTO/Mapper pattern; Created CustomRecord.java (DTO) with record CustomRecord<T>(String key, T message, ReceiverOffset acknowledgement) for structured data container; Created MessageConverter.java (mapper) with static toRecord() method extracting KafkaHeaders.RECEIVED_KEY, payload via getPayload(), and KafkaHeaders.ACKNOWLEDGMENT for ReceiverOffset; Modified KafkaConsumer.java to use .map(MessageConverter::toRecord) transformation and printMsgDetails(CustomRecord<String>) with manual rec.acknowledgement().acknowledge() for explicit offset commits; Updated application-section4.yaml with consumer group "some-group" and group.instance.id for static consumer instances; Demonstrates enterprise DTO/Mapper pattern separating Spring framework Message<T> from domain CustomRecord<T>; Message flow: Message<String> → MessageConverter.toRecord() → CustomRecord<String> → process payload/key → manual acknowledgment → offset commit; Enables precise message lifecycle control, prevents duplicate processing on restart, and provides clean separation between messaging framework and business logic
- Added: application-section4.yaml with key serialization configuration; Modified: application.yaml to use section4 profile
- created pkg: kafka.section4 with enhanced messaging using Message<T> wrapper pattern; KafkaProducer modified to produce Supplier<Flux<Message<String>>> using MessageBuilder for structured message creation with payload, Kafka keys (KafkaHeaders.KEY), and custom headers; KafkaConsumer modified to consume Consumer<Flux<Message<String>>> with printMsgDetails() method for extracting payload via msg.getPayload() and headers via msg.getHeaders(); enables rich metadata transport, message partitioning control, and distributed tracing capabilities; message flow: toMessage() → MessageBuilder.withPayload() → setHeader(KafkaHeaders.KEY) → setHeader(custom) → build() → Flux<Message<String>> → Kafka topic → consumer → printMsgDetails() → extract payload/headers for processing
- [BP] refactor: implement industry best practices for KafkaProcessorTest with hybrid approach; Extract KafkaProcessorTestConfiguration as separate class with @TestConfiguration; Implement hybrid pattern: encapsulated Sinks for input, ConcurrentLinkedQueue for output; Add clean public interface with emitMessage() method for controlled message emission; Replace tight coupling with proper separation of concerns using @ContextConfiguration; Add proper test isolation with @BeforeEach cleanup and queue clearing; Enhance error messages with queue contents for improved debugging; Implement message flow: emitMessage() → inputSink → testProducer → processor → testConsumer → verification;  Add testKafkaProcessorWithSingleMessage() for edge case coverage; Fix timing issues ensuring messages flow after Spring context initialization; solve reactive stream timing issue in previous commit
- created KafkaProcessorTest. Coded original version of integration test using sinks; modified AIT to move properties from test impl (KafkaConsumer/ProducerTest) to AIT (logging, offset.reset)
- created scf KafkaProcessor (kafka.section3) to run within a producer-processor-consumer pipeline -> KafkaProducer with producer():Supplier<Flux<String>> emitting periodic messages; KafkaProcessor with processor():Function<Flux<String>,Flux<String>> using flatMap for concurrent message transformation (toUpperCase); KafkaConsumer with consumer():Consumer<Flux<String>> for final consumption; configured application-section3.yaml with function definition (producer;consumer;processor), topic bindings (input-topic → processor → output-topic), and consumer groups (processor-group, consumer-group); updated application.yaml to use section3 profile
- created KafkaProducerTest & KafkaProducerTestConfiguration; Separate test configuration from test logic using @ContextConfiguration;Implement thread-safe message capture with ConcurrentLinkedQueue; Add proper test isolation with @BeforeEach cleanup; Use reactive testing with StepVerifier and Mono.delay(); Remove tight coupling between test and configuration classes; Maintain clean separation of concerns for better maintainability 
- refactored KafkaConsumerTest to extract config into separate KafkaConsumerTestConfiguration class and referenced via @ContextConfiguration
- modified KafkaConsumerTest to add @TestPropertySource, set properties for: scf.definition, scs.bindings.testProducer-out-0.destination, logging.level.root/jayslabs.kafka. test currently defined for KafkaConsumer.consumer()
- src/test/java: create AbstractIntegrationTest (AIT) base test class (uses @EmbeddedKafka, @SpringbootTest, EmbeddedKafkaBroker - @Autowired); created KafkaConsumerTest (extends AIT, @ExtendWith OutputCaptureExtension) with @Test method testKafkaConsumer(CapturedOutput) - uses: Mono, .delay(), .then(), .fromSupplier(), Duration, output::getOut, Duration, .as(), StepVerifier, consumeNextWith(); Inner class @TestConfiguration TestConfig with testProducer():Supplier<Flux<String>>
- prep workspace for integration test: removed dep:scs-test-binder, added SCSAppTest using @EmbeddedKafka
- added code for setting binding properties via @Bean via SenderOptionsCustomizer (deprecated)
- created KafkaProducer with producer():Supplier<Flux<String>>; modified app.yaml to add to scf.definition, scs.bindings (producer-out-0)
- modified app.yaml to set properties based on kafka.binding (function-0, consumer-in-0)
- added code for setting binding properties via @Bean via ReceiverOptionsCustomizer (deprecated)
- modified app.yaml to define binder specific properties: spring.cloud.stream.kafka.binder.<configuration/producer-properties/consumer-properties>, set "group.instance.id" var
- modified app.yaml to setup for active profiles, + application-section2.yaml, + application.yaml, modified sping app to use scanBasePackages appending active profile var ${sec}
- modified KafkaConsumer to add another function bean - function():Function<Flux<String>,Mono<Void>>; modified application.yaml: added binding for function(), and set spring.cloud.function.definition to use function
- pkg: kafka.section2: created KafkaConsumer (@Configuration) with consumer():Consumer<Flux<String>> (@Bean); defined bindings in application.yaml: spring.cloud.stream.bindings
- initial project commit; updated pom reference for spring-cloud-stream-binder-kafka-reactive; readme update

#### proj folder: kafka-setup
- added docker-compose.yaml to setup docker (image: vinsdocker/kafka), volumes r
eferences server.properties; added /data/ to gitignore

#### repo: scs-reactive-kafka-microservices