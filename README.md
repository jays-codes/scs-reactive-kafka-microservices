# scs-reactive-kafka-microservices
Jay's project/practice repo for Event-driven Microservices using Reactive Kafka and Spring Cloud Stream

#### proj scs-kafka-sandbox (jayaslabs.kafka; SpringBoot 3.5.4, jdk 21; Clud Stream, Spring for Apache Kafka, Lombok, spring-cloud-stream-binder-kafka-reactive)

- demonstrate section2: Spring Cloud Stream automatic message wrapping capability; Modified KafkaConsumer.java consumer() bean from Consumer<Flux<String>> to Consumer<Flux<Message<String>>> while keeping KafkaProducer.java unchanged with Supplier<Flux<String>> (simple String payload); Added import org.springframework.messaging.Message; Updated application.yaml to use section2 profile; Demonstrates framework's intelligent auto-wrapping where simple String producers can be consumed as Message<String> objects; Producer sends: "message-4" (String) → Framework auto-wraps → Consumer receives: Message<String> with full metadata (payload, headers, partitionId, offset, acknowledgment, but kafka_receivedMessageKey=null since no key set); Message flow: Flux<String> → SCS auto-wrapping → Message<String> creation → Kafka topic → Message<String> consumer → access to acknowledgment/headers; Shows asymmetric producer-consumer pattern enabling backward compatibility, gradual migration paths, and consumer-side acknowledgment capabilities regardless of producer complexity; Validates that simple payloads work seamlessly with Message-aware consumers for maximum framework flexibility
- enhanced section4: implement manual message acknowledgment with DTO/Mapper pattern; Created CustomRecord.java (DTO) with record CustomRecord<T>(String key, T message, ReceiverOffset acknowledgement) for structured data container; Created MessageConverter.java (mapper) with static toRecord() method extracting KafkaHeaders.RECEIVED_KEY, payload via getPayload(), and KafkaHeaders.ACKNOWLEDGMENT for ReceiverOffset; Modified KafkaConsumer.java to use .map(MessageConverter::toRecord) transformation and printMsgDetails(CustomRecord<String>) with manual rec.acknowledgement().acknowledge() for explicit offset commits; Updated application-section4.yaml with consumer group "some-group" and group.instance.id for static consumer instances; Demonstrates enterprise DTO/Mapper pattern separating Spring framework Message<T> from domain CustomRecord<T>; Message flow: Message<String> → MessageConverter.toRecord() → CustomRecord<String> → process payload/key → manual acknowledgment → offset commit; Enables precise message lifecycle control, prevents duplicate processing on restart, and provides clean separation between messaging framework and business logic
- Added: application-section4.yaml with key serialization configuration; Modified: application.yaml to use section4 profile
- created pkg: kafka.section4 with enhanced messaging using Message<T> wrapper pattern; KafkaProducer modified to produce Supplier<Flux<Message<String>>> using MessageBuilder for structured message creation with payload, Kafka keys (KafkaHeaders.KEY), and custom headers; KafkaConsumer modified to consume Consumer<Flux<Message<String>>> with printMsgDetails() method for extracting payload via msg.getPayload() and headers via msg.getHeaders(); enables rich metadata transport, message partitioning control, and distributed tracing capabilities; message flow: toMessage() → MessageBuilder.withPayload() → setHeader(KafkaHeaders.KEY) → setHeader(custom) → build() → Flux<Message<String>> → Kafka topic → consumer → printMsgDetails() → extract payload/headers for processing
- [BP] refactor: implement industry best practices for KafkaProcessorTest with hybrid approach; Extract KafkaProcessorTestConfiguration as separate class with @TestConfiguration; Implement hybrid pattern: encapsulated Sinks for input, ConcurrentLinkedQueue for output; Add clean public interface with emitMessage() method for controlled message emission; Replace tight coupling with proper separation of concerns using @ContextConfiguration; Add proper test isolation with @BeforeEach cleanup and queue clearing; Enhance error messages with queue contents for improved debugging; Implement message flow: emitMessage() → inputSink → testProducer → processor → testConsumer → verification;  Add testKafkaProcessorWithSingleMessage() for edge case coverage; Fix timing issues ensuring messages flow after Spring context initialization; solve reactive stream timing issue in previous commit
- created KafkaProcessorTest. Coded original version of integration test using sinks; modified AIT to move properties from test impl (KafkaConsumer/ProducerTest) to AIT (logging, offset.reset)
- created scf KafkaProcessor (kafka.section3) to run within a producer-processor-consumer pipeline -> KafkaProducer with producer():Supplier<Flux<String>> emitting periodic messages; KafkaProcessor with processor():Function<Flux<String>,Flux<String>> using flatMap for concurrent message transformation (toUpperCase); KafkaConsumer with consumer():Consumer<Flux<String>> for final consumption; configured application-section3.yaml with function definition (producer;consumer;processor), topic bindings (input-topic → processor → output-topic), and consumer groups (processor-group, consumer-group); updated application.yaml to use section3 profile
- created KafkaProducerTest & KafkaProducerTestConfiguration; Separate test configuration from test logic using @ContextConfiguration;Implement thread-safe message capture with ConcurrentLinkedQueue; Add proper test isolation with @BeforeEach cleanup; Use reactive testing with StepVerifier and Mono.delay(); Remove tight coupling between test and configuration classes; Maintain clean separation of concerns for better maintainability 
- refactored KafkaConsumerTest to extract config into separate KafkaConsumerTestConfiguration class and referenced via @ContextConfiguration
- modified KafkaConsumerTest to add @TestPropertySource, set properties for: scf.definition, scs.bindings.testProducer-out-0.destination, logging.level.root/jayslabs.kafka. test currently defined for KafkaConsumer.consumer()
- src/test/java: create AbstractIntegrationTest (AIT) base test class (uses @EmbeddedKafka, @SpringbootTest, EmbeddedKafkaBroker - @Autowired); created KafkaConsumerTest (extends AIT, @ExtendWith OutputCaptureExtension) with @Test method testKafkaConsumer(CapturedOutput) - uses: Mono, .delay(), .then(), .fromSupplier(), Duration, output::getOut, Duration, .as(), StepVerifier, consumeNextWith(); Inner class @TestConfiguration TestConfig with testProducer():Supplier<Flux<String>>
- prep workspace for integration test: removed dep:scs-test-binder, added SCSAppTest using @EmbeddedKafka
- added code for setting binding properties via @Bean via SenderOptionsCustomizer (deprecated)
- created KafkaProducer with producer():Supplier<Flux<String>>; modified app.yaml to add to scf.definition, scs.bindings (producer-out-0)
- modified app.yaml to set properties based on kafka.binding (function-0, consumer-in-0)
- added code for setting binding properties via @Bean via ReceiverOptionsCustomizer (deprecated)
- modified app.yaml to define binder specific properties: spring.cloud.stream.kafka.binder.<configuration/producer-properties/consumer-properties>, set "group.instance.id" var
- modified app.yaml to setup for active profiles, + application-section2.yaml, + application.yaml, modified sping app to use scanBasePackages appending active profile var ${sec}
- modified KafkaConsumer to add another function bean - function():Function<Flux<String>,Mono<Void>>; modified application.yaml: added binding for function(), and set spring.cloud.function.definition to use function
- pkg: kafka.section2: created KafkaConsumer (@Configuration) with consumer():Consumer<Flux<String>> (@Bean); defined bindings in application.yaml: spring.cloud.stream.bindings
- initial project commit; updated pom reference for spring-cloud-stream-binder-kafka-reactive; readme update

#### proj folder: kafka-setup
- added docker-compose.yaml to setup docker (image: vinsdocker/kafka), volumes r
eferences server.properties; added /data/ to gitignore

#### repo: scs-reactive-kafka-microservices