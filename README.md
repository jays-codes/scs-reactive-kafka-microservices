# scs-reactive-kafka-microservices
Jay's project/practice repo for Event-driven Microservices using Reactive Kafka and Spring Cloud Stream

#### proj: saga-choreo (jayslabs.kafka; SpringBoot 3.5.6, jdk 21; Cloud Stream, Spring for Apache Kafka, Lombok, Spring Reactive Web, Spring Data R2DBC, H2, spring-cloud-stream-binder-kafka-reactive)

##### Description
Each service module configured with parent POM inheritance for consistent dependency management; Project demonstrates event-driven microservices architecture using choreography pattern where services react to events without central orchestrator; Architecture: services observe domain events (OrderCreated, PaymentDeducted, InventoryDeducted, ShippingScheduled) and react independently with parallel processing model; Each service maintains its own H2 database (R2DBC for reactive persistence) and publishes events via Kafka topics; Pattern enables distributed transaction management through compensating transactions (e.g., PaymentRefunded when inventory fails); Foundation for building production-grade event-driven microservices
with saga pattern for distributed transaction management


##### Technology Stack:
- Spring Cloud Stream (event-driven abstraction)
- Reactive Kafka Binder (Kafka integration)
- Spring WebFlux (reactive web layer)
- Spring Data R2DBC (reactive database access)
- H2 Database (embedded for development)
- Lombok (boilerplate reduction)

##### Architecture Pattern: Saga Choreography
- Decentralized event-driven coordination
- Services react to domain events without central orchestrator
- Parallel processing model (services observe OrderCreated simultaneously)
- Each service maintains independent database (database-per-service pattern)
- Compensating transactions for rollback (e.g., PaymentRefunded on inventory failure)

##### Module Responsibilities:
- choreo-common: Shared DTOs, domain events, utilities
- order-service: Order workflow coordination, order state management
- customer-payment: Payment processing, refund compensation
- inventory-service: Stock reservation, inventory restoration
- shipping-service: Delivery scheduling

##### Changes
- [order-service] created messaging layer event mappers (pkg jayslabs.kafka.order.messaging.mapper) for bidirectional event-DTO transformations maintaining clean boundaries between messaging and service layers; OrderEventMapper provides outbound mappings: toOrderCreatedEvent(PurchaseOrderDTO):OrderEvent.OrderCreated, toOrderCancelledEvent(), toOrderCompletedEvent(); PaymentEventMapper provides inbound mappings consuming PaymentEvent types: toOrderPaymentDTO(PaymentDeducted), toOrderPaymentDTO(PaymentFailed) captures failure message, toOrderPaymentDTO(PaymentRefunded); InventoryEventMapper provides inbound mappings: toOrderInventoryDTO(InventoryDeducted), toOrderPaymentDTO(InventoryFailed), toOrderInventoryDTO(InventoryRestored); ShippingEventMapper provides inbound mapping: toOrderShipmentDTO(ShippingScheduled); Renamed OrderShippingDTO → OrderShipmentDTO for consistency
- [BP] implemented completeOrder() in OrderFulfillmentServiceImpl with database-level aggregation logic; Added getWhenOrderComponentsAreSuccess(orderId) custom query method to PurchaseOrderRepository using @Query with SQL EXISTS subquery checking ALL component statuses in single database roundtrip: Step 1 (Query with Conditions) uses custom SQL query validating three conditions atomically: (a) po.status = 'PENDING' ensures only pending orders eligible for completion, (b) EXISTS subquery with INNER JOIN between order_payment and order_inventory tables on order_id, (c) op.success AND oi.success ensures BOTH components succeeded; completeOrder() implements 4-step reactive pipeline: Step 1 (Atomic Query) uses porepo.getWhenOrderComponentsAreSuccess(orderId) returning Mono<PurchaseOrder> only if ALL conditions met (empty Mono if any condition fails), Step 2 (Update In-Memory) uses .doOnNext(entity.setStatus(COMPLETED)) for status mutation, Step 3 (Persist) uses .flatMap(save) for UPDATE query, Step 4 (Transform) uses .map(toPurchaseOrderDTO) for entity→DTO conversion;
- [BP] created <<OrderFulfillmentService>> and OrderFulfillmentServiceImpl (@Service) for centralized order status decision-making in saga choreography; Interface defines completeOrder(orderId) and cancelOrder(orderId) contracts; cancelOrder() implements simple reactive pipeline: Step 1 (Query) uses porepo.findByOrderIdAndStatus(orderId, PENDING) for idempotent status guard ensuring only PENDING orders can be cancelled, Step 2 (Update In-Memory) uses .doOnNext(entity.setStatus(CANCELLED)) for status mutation, Step 3 (Persist) uses .flatMap(save) for UPDATE query, Step 4 (Transform) uses .map(toPurchaseOrderDTO) for entity→DTO conversion; completeOrder()
- implemented ShippingComponentServiceImpl following same pattern as for previous component service implementations. Fetcher interface is n/a as shipping data is not in a separate table but in purchase_order; onFailure() and onRollback() is n/a as well
- implemented InventoryComponentServiceImpl with exact same flow as PaymentComponentServiceImpl. Changes are dto used is OrderInventoryDTO, repo is OrderInventoryRepository
- [BP] implemented PaymentComponentServiceImpl (@Service) as dual-purpose saga participant implementing PaymentComponentFetcher (read side) and PaymentComponentStatusListener (write side); Dependency: OrderPaymentRepository; DEFAULT_DTO constant (OrderPaymentDTO.builder().build()) provides fallback for graceful degradation when payment not yet processed; getComponent(orderId) implements read side using 3-step pipeline: Step 1 (Query) uses pymtrepo.findByOrderId() for SELECT by orderId, Step 2 (Transform) uses .map(toOrderPaymentDTO) for entity→DTO conversion, Step 3 (Fallback) uses .defaultIfEmpty(DEFAULT_DTO) ensuring Mono always emits value (never empty) for aggregation in OrderServiceImpl.getOrderDetails(); onSuccess(OrderPaymentDTO) handles PaymentEvent.PaymentDeducted implementing idempotent INSERT using query-before-insert pattern: Step 1 (Idempotency Check) uses findByOrderId() to detect duplicates, Step 2 (Conditional Insert) uses .switchIfEmpty(Mono.defer(() -> add(event, true))) for lazy evaluation preventing unnecessary add() calls, Step 3 (Discard) uses .then() converting Mono<OrderPayment> to Mono<Void>; onFailure(OrderPaymentDTO) handles PaymentEvent.PaymentFailed with identical logic to onSuccess() except add(event, false) sets success=false flag for failed payments; onRollback(OrderPaymentDTO) handles PaymentEvent.PaymentRefunded implementing UPDATE operation: add(dto, isSuccess) helper method performs DTO→entity transformation via EntityDTOMapper.toOrderPayment(), sets success flag, executes pymtrepo.save() for INSERT; Pattern demonstrates materialized view (payment-service is source of truth, order-service maintains local view via events), idempotency via query-before-insert, lazy evaluation via Mono.defer() for performance, graceful degradation via DEFAULT_DTO, eventually consistent reads; Added inline flow diagrams for getComponent(), onSuccess(), and event timeline scenarios visualizing reactive pipelines, parallel execution, and saga coordination
- [BP] implemented OrderServiceImpl; Dependencies: PurchaseOrderRepository, OrderEventListener (emits OrderCreated events to Kafka for saga initiation), PaymentComponentFetcher (fetches payment status via getComponent(orderId) querying order_payment table), InventoryComponentFetcher (inventory status); placeOrder(OrderCreateRequest) implements reactive pipeline: Step 1 (Entity Creation) uses EntityDTOMapper.toPurchaseOrder() calculating amount (quantity * unitPrice) and setting status=PENDING, Step 2 (Database Persistence) uses porepo.save() for INSERT with auto-generated orderId, Step 3 (Entity→DTO Transformation) uses .map(toPurchaseOrderDTO) for immutability boundary, Step 4 (Saga Initiation) uses .doOnNext(ordEvtLstnr::emitOrderCreated) as non-blocking side effect emitting OrderEvent.OrderCreated to Kafka triggering parallel processing by payment/inventory/shipping services; getAllOrders() implements simple query using porepo.findAll().map(toPurchaseOrderDTO) streaming results as Flux<PurchaseOrderDTO> for UI table listing; getOrderDetails(orderId) implements component aggregation using 5-step reactive pipeline: Step 1 (Find Order) uses porepo.findById(orderId), Step 2 (Entity→DTO) uses .map(toPurchaseOrderDTO) capturing podto in closure, Step 3 (Fetch Payment) uses .flatMap(paymentCompFetcher.getComponent(orderId)) for reactive unwrapping, Step 4 (Zip with Inventory) uses .zipWith(inventoryCompFetcher.getComponent(orderId)) for parallel execution creating Tuple2<OrderPaymentDTO, OrderInventoryDTO>, Step 5 (Aggregate) uses .map(tup → toOrderDetailsDTO(podto, tup.T1, tup.T2)) combining order + payment + inventory into single DTO;  ShippingComponentFetcher omitted because deliveryDate stored in purchase_order table (denormalization for performance); Added inline flow diagrams for placeOrder() and getOrderDetails() visualizing reactive pipelines and parallel execution
- [BP] OrderService.java - updated to add commented Communication Flow diagram
- [BP] created component interfaces for saga coordination: payment package (PaymentComponentFetcher, PaymentComponentStatusListener), shipping package (ShippingComponentFetcher commented out as not required, ShippingComponentStatusListener), inventory package (InventoryComponentFetcher, InventoryComponentStatusListener); all extend OrderComponentFetcher<T> or OrderComponentStatusListener<T> for type-safe component access and status handling; added OrderShippingDTO (simplified to orderId, deliveryDate); 
- added Service inteface (order.common.service): <<OrderService>>, OrderDetailsDTO record
- [order-service] added choreo-common dependency to pom.xml; created repositories: PurchaseOrderRepository (with findByOrderIdAndStatus()), OrderPaymentRepository (with findByOrderId()), OrderInventoryRepository (with findByOrderId()); implemented EntityDTOMapper with bidirectional transformations: toPurchaseOrder(OrderCreateRequest) calculates amount and sets PENDING status, toPurchaseOrderDTO(PurchaseOrder), toOrderPayment(OrderPaymentDTO), toOrderPaymentDTO(OrderPayment), toOrderInventory(OrderInventoryDTO), toOrderInventoryDTO(OrderInventory); Pattern follows orderId-based saga correlation via status-based queries, static utility methods for stateless transformations, builder pattern for immutable construction
- [order-service] added data.sql
- implemented test artifacts at src/test/java at pkg: jayslabs.kafka.shipping: AbstractIntegrationTest, TestDataUtil, ShippingService
- implemented messaging layer (pkg jayslabs.kafka.shipping.messaging.*) mirroring customer-payment architecture; OrderEventProcessorImpl (@Service) implements OrderEventProcessor<ShippingEvent> with type-safe event routing via pattern matching: handle(OrderCreated) maps event to CreateShippingRequest via EventDTOMapper.toCreateShippingRequest(), delegates to service.createShipmentRecord(), applies exceptionHandler() for idempotency (EventAlreadyProcessedException → Mono.empty()), returns Mono.empty() (no outbound event for initial creation); handle(OrderCancelled) invokes service.cancelShipment(orderId) for compensating transaction, returns Mono.empty() (cleanup operation); handle(OrderCompleted) invokes service.scheduleShipment(orderId), maps ShipmentDTO to ShippingEvent.ShippingScheduled via EventDTOMapper.toShippingScheduledEvent(), emits outbound event signaling saga progression; OrderEventProcessorConfig (@Configuration) defines processor() bean (Function<Flux<Message<OrderEvent>>, Flux<Message<ShippingEvent>>>) consuming order-events topic, uses MessageConverter.toRecord() extracting CustomRecord<OrderEvent>, concatMap() for sequential processing preserving partition order, manual acknowledgment via doOnSuccess(acknowledge()), toMessage() helper sets KafkaHeaders.KEY=orderId for partitioning; EventDTOMapper (utility) provides static transformations: toCreateShippingRequest(OrderCreated) for inbound event→DTO, toShippingScheduledEvent(ShipmentDTO) for outbound DTO→event with Instant.now() timestamp; application.yaml
- [BP] implemented ShippingServiceImpl (@Service) with @Transactional reactive methods following saga choreography pattern; createShipmentRecord(CreateShippingRequest) implements 2-phase pipeline: Phase 1 (Idempotency) uses DuplicateEventValidator.validate() with shipmentRepo.existsByOrderId() preventing duplicate shipment creation, Phase 2 (Creation) uses Mono.defer() for lazy evaluation creating Shipment entity via EntityDTOMapper.toShipment(), sets status to PENDING, persists via shipmentRepo.save().then() returning Mono<Void>; scheduleShipment(orderId) implements 2-phase pipeline: Phase 1 (Lookup) uses findByOrderIdAndStatus(orderId, PENDING) ensuring only pending shipments schedulable, Phase 2 (Scheduling) updates status to SCHEDULED, sets deliveryDate to Instant.now().plus(7, ChronoUnit.DAYS), persists and maps to ShipmentDTO via EntityDTOMapper.toShipmentDTO(); cancelShipment(orderId) implements compensating transaction using deleteByOrderId() for saga rollback (triggered by OrderEvent.OrderCancelled or PaymentEvent.PaymentFailed), returns Mono<Void> with doOnNext() logging; Pattern mirrors PaymentServiceImpl architecture: orderId-based saga correlation (not shipmentId), DuplicateEventValidator for idempotency, status-based state transitions (PENDING→SCHEDULED), reactive error handling via Mono.empty() for no-op scenarios, comprehensive logging via doOnNext() for observability
- added ShippingService interface (common.service) defining saga-correlated contracts: createShipmentRecord(CreateShippingRequest) for initial shipment creation, cancelShipment(orderId) for compensating transaction (uses orderId as saga correlation ID matching OrderEvent.OrderCancelled payload), scheduleShipment(orderId) for delivery scheduling (triggered by InventoryEvent.InventoryDeducted); 
Modified ShipmentDTO to change field from id to shipmentid; reflected change to EntityDTOMapper; Added application.yaml configuring Spring Cloud Stream processor function consuming order-events (shipping-group consumer) and producing shipping-events, JSON serialization/deserialization with trusted packages for sealed interfaces; Pattern follows saga choreography where services use orderId (not entity IDs) for distributed transaction coordination via repository query findByOrderIdAndStatus(); Added deleteByOrderId() to ShipmentRepository
- Added: pkg structure; entity class: Shipment; <<ShipmentRepository>>; dto records: CreateShippingRequest, ShipmentDTO; class EntityDTOMapper with toShipment(CreateShippineReq):Shipment, toShipmentDTO(Shipment):ShipmentDTO 
- [shipping-service] added microservice schema: shipment table
- verified custom inventory-service microservice against recommended implementation
- [BP] implemented InventoryServiceTest, AbstractIntegrationTest and TestDataUtil in for Inventory-Service
- Implemented OrderEventProcessorConfig class (inventory.messaging.config) for InventoryService. processor is referenced in spring.cloud.stream.bindings in app.yaml; copied application.yaml and modified for inventory-service; created InventoryApplication (@SpringBootApplication) class in jayslabs.kafka.inventory
- Implemented OrderEventProcessorImpl class (inventory.messaging.processor) for InventoryService; modified <<OrderEventProcessor>> to set handle(OrderEvent.OrderComplete) to return Mono.empty()
- Implemented EventDTOMapper class (inventory.messaging.mapper)
- Implemented InventoryServiceImpl (inventory.application.service), interface <<InventoryService>> (inventory.common.service)
- Added DTO classes: InventoryDTO and InventoryProcessRequest, Added EntityDTOMapper, OutOfStockException
- [inventory-service] created package structure similar to customer-payment; Created Product and OrderInventory entity classes; created repositories <<ProductRepository>>, <<InventoryRepository>> 
- [BP] refactored PaymentServiceTest, added expectEvent(), and expectNoEvent()
- Added test scenarios: refundWithoutDeductTest(), customerNotFoundTest(), insufficientBalanceTest()
- Added test steps to deductAndRefundTest() to factor in the ff: check balance, check duplicate event, cancel and refund; created class reference to respFlux:Flux<PaymentEvent> 
- [BP] implemented PaymentServiceTest integration test validating complete reactive pipeline from Kafka to database; Test uses Sinks.Many pattern for controlled message emission (reqSink for OrderEvent input, respSink for PaymentEvent capture), @TestPropertySource configures Spring Cloud Stream functions (processor, orderEvtProducer, paymentEvtConsumer) with topic bindings (order-events input, payment-events output), TestConfig provides test infrastructure beans (Supplier<Flux<OrderEvent>> producer converting reqSink to Kafka messages, Consumer<Flux<PaymentEvent>> consumer capturing output to respSink); processPaymentTest() validates happy path validating paymentId generation, orderId correlation, and amount calculation; Test executes full stack: reqSink → orderEvtProducer → order-events topic → OrderEventProcessorConfig.processor() → OrderEventProcessorImpl.handle() → PaymentServiceImpl.processPayment() → EventDTOMapper → payment-events topic → paymentEvtConsumer → respSink; Enhanced test files with comprehensive inline documentation explaining reactive flow, message routing, and StepVerifier pattern; Fixed Maven version inheritance by removing explicit version declarations from choreo-common and customer-payment POMs (now inherit 0.0.1-SNAPSHOT from parent), resolved ClassNotFoundException for CustomRecord; Demonstrates production-grade integration testing: embedded Kafka (@EmbeddedKafka), full Spring context (@SpringBootTest), reactive stream testing (StepVerifier), controlled emission/capture (Sinks), isolation (@DirtiesContext)
- Unit Testing for customer-payment (src/test/java/jayslabs.kafka.payment): Added AbstractIntegrationTest, TestDataUtil
- Added application.yaml - Spring Cloud Stream configuration including:
Function definition for reactive processor, Kafka bindings (order-events input, payment-events output), JSON serialization/deserialization setup, Trusted packages for sealed interface support, Consumer group configuration; Added data.sql - Database schema initialization with: Customer table, Customer_payment table with foreign key, Test data seeding (anya, becky, bondo); added CustomerPaymentApplication class - Spring Boot entry point @SpringBootApplication replacing the placeholder
- [BP] created OrderEventProcessorConfig as Spring Cloud Stream integration bridge connecting Kafka messaging with reactive domain event processing; Created comprehensive PlantUML diagrams visualizing complete architecture: class diagram (order-event-processor-config-class.puml → OrderEventProcessorConfig Class Diagram.png) depicting structural relationships across messaging/service/repository/entity layers with two-column portrait layout showing OrderEventProcessorConfig → OrderEventProcessorImpl → PaymentServiceImpl → CustomerRepository/PaymentRepository → Customer/CustomerPayment entities, includes Spring Framework (Message<T>, MessageBuilder, Flux, Mono, ReceiverOffset) and Spring Data (ReactiveCrudRepository) dependencies; sequence diagrams split into Part 1 (order-event-processor-config-sequence.puml → Part 1.png) covering Scenario 1 (successful payment with 5-phase pipeline) and Scenario 2 (customer not found with three-tier exception handler transforming CustomerNotFoundException to PaymentEvent.PaymentFailed), Part 2 (order-event-processor-config-sequence-part2.puml → Part 2.png) covering Scenario 3 (duplicate event with silent swallow via EventAlreadyProcessedException → Mono.empty()) and Scenario 4 (order cancelled compensation with 4-phase refund pipeline using zipWhen for reactive joins and sequential persistence); Enhanced OrderEventProcessorConfig with inline documentation
- [BP] implemented OrderEventProcessorImpl.handle(OrderEvent.OrderCancelled) invoking service.processRefund(orderId) for compensating transaction execution, maps PaymentDTO response to PaymentEvent.PaymentRefunded via EventDTOMapper.toPaymentRefundedEvent() (added); added doOnNext() logging for refund audit trail and doOnError() for failure observability; Created EventDTOMapper.toPaymentRefundedEvent(PaymentDTO) transformation method building PaymentEvent.PaymentRefunded with orderId/paymentId/customerId/amount/createdAt preserving immutability boundaries; Implemented handle(OrderEvent.OrderCompleted) as no-op returning Mono.empty() for terminal saga event (no business logic required);
- [BP] implement reactive exception handling strategy in OrderEventProcessorImpl for saga choreography error recovery; Demonstrates production-grade patterns: graceful degradation (exceptions become events not crashes), separation of concerns (messaging layer handles Kafka errors, service layer throws domain exceptions), type-safe error transformation (Function<Throwable, Mono<T>>), closure capture for context preservation, saga choreography integration (PaymentFailed triggers compensating transactions); Enhanced handle(OrderEvent.OrderCreated) with .transform(exceptionHandler(event)) applying UnaryOperator<Mono<PaymentEvent>>; Implemented private exceptionHandler() method with three-tier strategy: Tier 1 (Idempotency Protection) uses .onErrorResume(EventAlreadyProcessedException.class, e -> Mono.empty()) for silent duplicate event swallowing preventing duplicate PaymentDeducted events in at-least-once Kafka delivery, Tier 2 (commented out specific handlers) shows optional explicit handling per exception type (.onErrorResume(CustomerNotFoundException.class, ...) / .onErrorResume(InsufficientBalanceException.class, ...)) for fine-grained control, Tier 3 (Catch-All Handler) uses .onErrorResume(EventDTOMapper.toPaymentFailedEvent(evt)) catching all remaining business exceptions (CustomerNotFoundException, InsufficientBalanceException, unexpected errors) transforming them into PaymentEvent.PaymentFailed domain events; Created EventDTOMapper.toPaymentFailedEvent(OrderEvent.OrderCreated evt) returning Function<Throwable, Mono<PaymentEvent>> preserving original event context (orderId, customerId, totalAmount) and exception message (ex.getMessage()); Uses Mono.fromSupplier() for lazy PaymentEvent.PaymentFailed creation maintaining reactive chain integrity; 
Added PlantUML sequence diagram (payment-exception-handling-flow.puml) visualizing four scenarios: successful payment (happy path with 5 phases), customer not found (Tier 3 catch-all recovery), insufficient balance (business rule violation), duplicate event (Tier 1 silent swallow); 
- [BP] wrote OrderEventProcessorImpl (payment.messaging.processor) implementing <OrderEventProcessor<PaymentEvent>>; Implemented handle(OrderEvent.OrderCreated) with sophisticated reactive pipeline; Created EventDTOMapper utility class with static transformation methods maintaining immutability boundaries between messaging layer (domain events) and service layer (DTOs); (payment.messaging.mapper) - toPaymentProcessRequest() transforms domain event (orderId, customerId, totalAmount) to service layer PaymentProcessRequest DTO maintaining clean layer boundaries; witin impl, invokes service.processPayment() to execute 5-phase reactive processing pipeline (idempotency check, customer validation, balance verification, payment deduction, audit logging), EventDTOMapper.toPaymentDeductedEvent() transforms PaymentDTO response to PaymentEvent.PaymentDeducted domain event with timestamp (createdAt=Instant.now()) for saga correlation, doOnNext() provides non-blocking side-effect logging for observability;  
- [BP] implement saga compensation with processRefund() and refundPayment() reactive pipeline for distributed transaction rollback; Enhanced PaymentServiceImpl with @Transactional processRefund(UUID orderId) method implementing sophisticated reactive flow: Phase 1 finds payment via pymtRepo.findByOrderIdAndStatus(orderId, PaymentStatus.DEDUCTED) ensuring only deducted payments qualify for refund (implicit idempotency through status guard), Phase 2 uses zipWhen() operator to reactively join CustomerPayment with Customer data creating Tuple2<CustomerPayment, Customer> for coordinated access to both entities, Phase 3 executes flatMap() transformation calling private refundPayment() method with tuple decomposition (tup.getT1(), tup.getT2()), Phase 4 logs refund completion via doOnNext() for audit trail; Implemented refundPayment(CustomerPayment, Customer) private method performing compensating transaction: updates customer balance in-memory (balance + amount), sets payment status to PaymentStatus.REFUNDED, chains custRepo.save().then(pymtRepo.save()) for sequential persistence ensuring customer balance restores before payment status updates, maps entity to PaymentDTO; Demonstrates advanced reactive patterns: zipWhen() for reactive joins avoiding nested flatMap chains, then() operator for sequential execution maintaining transaction consistency, Tuple2 for preserving multiple entity references; Included notes as comments in code
- [BP] implement PaymentServiceImpl with reactive payment processing pipeline and comprehensive inline documentation; Created @Service implementation with @Transactional processPayment() method orchestrating multi-phase reactive flow: Phase 1 (Idempotency) uses DuplicateEventValidator.validate() with pymtRepo.existsByOrderId() to prevent duplicate event processing, Phase 2 (Customer Validation) uses custRepo.findById().switchIfEmpty(CUSTOMER_NOT_FOUND) for existence check, Phase 3 (Balance Validation) uses .filter(balance >= amount).switchIfEmpty(INSUFFICIENT_BALANCE) for fund verification, Phase 4 (Payment Deduction) uses .flatMap(deductPayment()) for atomic transaction execution, Phase 5 (Logging) uses .doOnNext() for audit trail; Implemented deductPayment() private method performing coordinated database writes: creates CustomerPayment entity via EntityDTOMapper, updates customer balance in-memory, sets PaymentStatus.DEDUCTED, chains custRepo.save().then(pymtRepo.save()) for sequential persistence, maps entity to immutable PaymentDTO; Uses static error Monos (CUSTOMER_NOT_FOUND, INSUFFICIENT_BALANCE) for performance optimization avoiding repeated object creation; Added comprehensive inline comments documenting reactive flow phases, validation logic, and database operations; Added multi-line block comment visualizing complete reactive pipeline with ASCII diagrams showing happy path flow through all 5 phases with example data (customerId=1, orderId=abc-123, amount=50); Renamed PayymentServiceImpl → PaymentServiceImpl 
- Created PaymentService interface (common.service) defining reactive contracts: processPayment(PaymentProcessRequestDTO) → Mono<PaymentDTO> for payment deduction, refundPayment(UUID orderId) → Mono<PaymentDTO> for compensating transaction; Implemented EntityDTOMapper (application.mapper) with static utility methods: toCustomerPayment(PaymentProcessRequestDTO) for DTO-to-entity conversion using builder pattern, toPaymentDTO(CustomerPayment) for entity-to-DTO conversion maintaining immutability boundary; Enhanced CustomerPayment entity with Lombok annotations (@Data, @Builder, @NoArgsConstructor, @AllArgsConstructor) enabling builder pattern and JPA compatibility; Created domain-specific exceptions (common.exception): CustomerNotFoundException for invalid customer ID scenarios, InsufficientBalanceException for balance validation failures
- [customer-payment] service implementation : Created package structure -> application layer (entity/repository/service/mapper), common layer (dto/exception/service), messaging layer (event handlers); Implemented entities: Customer (@Data, fields: id/name/balance) for customer account management, CustomerPayment (fields: paymentId/orderId/customerId/status/amount) for payment transaction audit trail; Created DTOs: PaymentDTO (@Builder record with paymentId/orderId/customerId/amount/status) for service-to-messaging communication, PaymentProcessRequestDTO (@Builder record with customerId/orderId/amount) for messaging-to-service requests; Implemented repositories: CustomerRepository extends ReactiveCrudRepository<Customer, Integer> for customer balance operations, PaymentRepository extends ReactiveCrudRepository<CustomerPayment, UUID> with custom existsByOrderId(UUID) query for idempotency checking; Architecture enforces immutability at boundaries (DTOs as records), confines mutable entities to service layer, decouples Kafka concerns from business logic; Added ARCHITECTURE_NOTES.md;
- added CustomRecord (record) and MessageConverter to common.util
- added in-code notes in comments to clarify reactive flow in duplicate validation
- [BP] added validator for Duplicate Events; pkgs common.util/exception; new classes: EventAlreadyProcessedException, DuplicateEventValidator; added sequence diagrams - DuplicateEventValidationFlow.png 
- created pkg common.publisher, and <<EventPublisher>>
- Implement same as for OrderEventProcessor below and create InventoryEventProcessor, PaymentEventProcessor, ShippingEventProcessor to process their respective DomainEvents; added missinh InventoryStatus enum
- [BP] implement type-safe event processor pattern with pattern matching for sealed interface event handling; Created two-layer interface hierarchy: EventProcessor<T extends DomainEvent, R extends DomainEvent> (generic abstraction with Mono<R> process(T) method), OrderEventProcessor<R extends DomainEvent> (specialized for OrderEvent input with generic output), implements default process() method using [BP] pattern matching switch (pms) routing to abstract handle() methods; pms handles all sealed OrderEvent types (OrderCreated, OrderCancelled, OrderCompleted) routing to strongly-typed handle() methods; overloaded handle() to provide template for handling diff OrderEvents: OrderEvent.OrderCreated/OrderCancelled/OrderCompleted forcing implementers to explicitly handle all event types; Created common.processor
- Implemented Shipping Domain Event; created pkg common.events.shipping; created ShippingEvent sealed interface; add inner records to represent event: ShippingScheduled; used @Builder (lombok); created enum ShippingStatus to represent states: PENDING, SCHEDULED, FAILED
- Implemented Inventory Domain Events; created pkg common.events.inventory; created InventoryEvent sealed interface; add inner records to represent events: InventoryDeducted, InventoryRestored, InventoryFailed; used @Builder (lombok); created enum InventoryStatus to represent states: DEDUCTED, RESTORED, FAILED
- Did same as below but for Payment Domain Events; created pkg common.events.payment; created PaymentEvent sealed interface; add inner records to represent events: PaymentDeducted, PaymentRefunded, PaymentFailed; used @Builder (lombok); created enum PaymentStatus to represent states: DEDUCTED, REFUNDED, FAILED
- [BP] implement sealed OrderEvent interface with inner record implementations for type-safe domain events; sealed interface contains three inner records: OrderCreated (full order details: orderId, productId, customerId, quantity, unitPrice, totalAmount, createdAt), OrderCancelled (minimal: orderId, message, createdAt), OrderCompleted (ultra-minimal: orderId, createdAt); Applied @Builder annotation to all records enabling fluent builder pattern for readable event construction; Created OrderStatus enum (PENDING, COMPLETED, CANCELLED) for order state management in database and REST APIs; 
- [choreo-common] created event-driven foundation for saga choreography in choreo-common module; Established event interface hierarchy with base Saga marker interface, OrderSaga interface (extends Saga, defines getOrderId():UUID for order correlation), DomainEvent interface (defines createdAt():Instant for temporal tracking), and OrderEvent interface (in events.order package, extends both DomainEvent and OrderSaga combining saga correlation with domain event semantics); Created package structure jayslabs.kafka.common.events with order subpackage for domain-specific events; Removed Main.java from choreo-common; [BP] Pattern demonstrates event-driven architecture foundation: marker interfaces for saga identification, temporal attributes for event sourcing, composition pattern enabling events to participate in both saga orchestration and domain event publishing; Foundation enables concrete event implementations (OrderCreated, PaymentDeducted, InventoryDeducted, ShippingScheduled) to inherit saga correlation ID (orderId) and timestamp (createdAt) for distributed tracing and event ordering across microservices
- refactor saga-choreo POM structure for cleaner dependency management; Removed duplicate dependencies from parent POM (saga-choreo/pom.xml), keeping only Lombok in parent <dependencies> section while service-specific dependencies (spring-boot-starter-data-r2dbc, webflux, spring-cloud-stream, spring-kafka, spring-cloud-stream-binder-kafka-reactive, H2, r2dbc-h2, test dependencies) remain inherited via dependencyManagement from spring-boot-starter-parent; Configured choreo-common/pom.xml with spring-cloud-stream-binder-kafka-reactive dependency using <scope>provided</scope> (avoiding unnecessary packaging in library module) and added spring-boot.repackage.skip=true property to prevent Spring Boot repackaging for shared library module; Fixed indentation inconsistencies in customer-payment/pom.xml <dependencies> section ensuring uniform 8-space indentation for all dependency blocks; Pattern demonstrates Maven best practice: parent POM defines only truly shared dependencies while child modules declare their specific needs, common library modules use provided scope for compile-time dependencies, and repackage skipping for non-executable modules
- initialize saga-choreo multi-module Maven project for implementing Saga Pattern (Choreography); Created parent POM (saga-choreo) with Spring Boot 3.5.6, Spring Cloud Stream 2025.0.0, reactive Kafka binder, WebFlux, R2DBC with H2, and Lombok dependencies; with five modules: choreo-common (shared DTOs/events), order-service (order workflow coordinator), customer-payment (payment/refund processing), inventory-service (stock management), shipping-service (delivery scheduling); 

#### proj: scs-kafka-sandbox (jayslabs.kafka; SpringBoot 3.5.5, jdk 21; Cloud Stream, Spring for Apache Kafka, Lombok, spring-cloud-stream-binder-kafka-reactive)

- section12: demonstrate multi-topic consumer pattern for single consumer consuming from multiple Kafka topics using comma-separated destination list; Created KafkaConsumer.java with simple Consumer<Flux<String>> consuming from both input-topic1 and input-topic2 via single consumer-in-0 binding; Configured application-section12.yaml with destination: "input-topic1,input-topic2" showing Spring Cloud Stream's built-in multi-topic subscription capability; Created MultiTopicConsumerTest.java integration test with two Sinks.Many<String> (sink1, sink2) simulating producers for producer1 (→ input-topic1) and producer2 (→ input-topic2); Test validates single consumer receives messages from multiple topics in merged order; Key difference from Fan-In pattern (section10): Multi-topic assumes same message type across topics with automatic merge by framework, while Fan-In uses Tuple2<Flux<T1>, Flux<T2>> for different types with explicit Flux.combineLatest() control; Message flow: sink1 → input-topic1 → consumer (merged), sink2 → input-topic2 → consumer (merged); Alternative approach: ReceiverOptionsCustomizer with subscription(List<String> topics) for programmatic multi-topic configuration; Use cases: aggregating logs from multiple services, consuming events from regional topics, merging audit trails; Enterprise pattern for simplified multi-source consumption when topics share same schema and processing logic doesn't require per-topic awareness
- section11: applied default scs (not kafka) properties in yaml, at scs level, for encoding/decoding, removing from binding level
- section11: [BP] successfully resolve polymorphic type deserialization with proper JsonSerializer/JsonDeserializer configuration; Fixed ClassCastException by configuring application-section11.yaml with explicit JsonDeserializer (kafka.binder.consumer-properties.value.deserializer) and critical spring.json.trusted.packages property set to "jayslabs.kafka.section11.dto" for security whitelisting; Configured EncodingDecodingTest with useNativeEncoding=true and JsonSerializer for producer via spring.cloud.stream.kafka.bindings.cmProducer-out-0.producer.configuration.value.serializer; Consumer now successfully deserializes abstract ContactMethod interface into concrete Email and Phone implementations; Demonstrates solution: JsonSerializer preserves concrete type information in JSON payload, JsonDeserializer reconstructs proper types when trusted packages configured; Key insight: spring.json.trusted.packages is mandatory security feature preventing arbitrary class deserialization (prevents deserialization attacks); Message flow: Flux<ContactMethod> (Email/Phone) → JsonSerializer (preserves type) → input-topic → JsonDeserializer (with trusted packages) → Consumer<Flux<ContactMethod>> successfully receives concrete types; Shows enterprise pattern: proper JSON serialization configuration enables polymorphic message handling without Jackson @JsonTypeInfo annotations when using Spring's JsonSerializer/JsonDeserializer; Validates that default Spring Kafka JSON serialization can handle interfaces/abstract types with correct configuration
- section11: [BP] demonstrate encoding/decoding challenges with abstract types and polymorphic message handling in Kafka; Created dto package with ContactMethod interface and concrete implementations (Email record, Phone record) to model polymorphic message types; Created KafkaConsumer.java consuming Consumer<Flux<ContactMethod>> to receive abstract type messages; Created EncodingDecodingTest.java with testEncodingDecodingForAbstractTypes() test and inner TestConfig providing cmProducer bean that emits Flux.just(new Email("test@test.com"), new Phone(14167)); Configured application-section11.yaml with consumer binding cmConsumer-in-0 for input-topic (deserializer config commented out for demonstration); Current implementation throws ClassCastException during deserialization: Spring Cloud Stream's default JSON serialization cannot properly deserialize abstract types without type metadata; Demonstrates critical challenge: when producer sends interface/abstract type (ContactMethod), Kafka stores JSON without concrete type information, causing consumer to fail reconstruction; Shows limitation of default JsonSerializer/JsonDeserializer with polymorphic types - requires Jackson @JsonTypeInfo annotations or custom serializer/deserializer with type hints; Message flow: Flux<ContactMethod> (Email/Phone) → JSON serialization (loses type info) → input-topic → JSON deserialization attempts ContactMethod instantiation → ClassCastException (cannot instantiate interface); Illustrates enterprise pattern challenge for event-driven systems with polymorphic domain models requiring type-safe serialization strategies (Jackson polymorphic type handling, Avro schema evolution, or Protocol Buffers)
- section11: demonstrate native encoding/decoding compatibility and serializer/deserializer configuration in Kafka messaging; Created KafkaConsumer.java consuming Flux<Integer> to show type-specific deserialization; Created EncodingDecodingTest.java using reactive KafkaSender with IntegerSerializer for both key and value to send Integer messages directly to Kafka; Test emits Integer values (1,2,3) to input-topic using createSender() and toSenderRecord() utilities from AbstractIntegrationTest; Demonstrates critical encoding/decoding principle: consumer can only decode messages if producer's serialization format matches consumer's expected type - IntegerSerializer (producer) → IntegerDeserializer (consumer); Configured application-section11.yaml with explicit deserializer properties: kafka.binder.consumer-properties with key.deserializer (IntegerDeserializer) and value.deserializer (IntegerDeserializer) to match producer's IntegerSerializer; Shows importance of serializer/deserializer configuration in Kafka: mismatched encodings (e.g., StringSerializer producer with IntegerDeserializer consumer) cause deserialization failures; Without proper deserializer configuration, Spring Cloud Stream cannot automatically convert binary data to Integer type; Message flow: Flux.range(1,3) → KafkaSender (IntegerSerializer) → input-topic → IntegerDeserializer (configured in YAML) → Consumer<Flux<Integer>>; Demonstrates enterprise pattern for type-safe Kafka messaging, explicit serialization/deserialization configuration, and troubleshooting encoding/decoding issues in distributed systems
- section10: [BP] implement Fan-In pattern for merging multiple input streams into single output; Created HeatIndexCalculator.java demonstrating Tuple2 input signature Function<Tuple2<Flux<Integer>, Flux<Integer>>, Flux<Long>> for consuming from two independent topics (temperature-topic and humidity-topic) and producing heat index calculations; Uses Flux.combineLatest() reactive operator to merge two input streams and apply heat index formula whenever either temperature or humidity changes; Implements NOAA heat index algorithm with 9-coefficient polynomial calculation (c1-c9) combining temperature and humidity values; Created FanInTest.java integration test with three Sinks (tempSink, humidSink, hiSink) validating reactive stream combination and calculation accuracy; Test emits temperature/humidity values and verifies heat index output: (90°F, 55%) → 97, (90°F, 60%) → 100, (94°F, 60%) → 110; Configured bindings for processor-in-0 (temp-topic), processor-in-1 (humid-topic), processor-out-0 (heat-index-topic); Message flow: Tuple2<Flux<temp>, Flux<humid>> → Flux.combineLatest() → heat index calculation → Flux<Long> → heat-index-topic; Demonstrates enterprise Fan-In pattern for sensor data fusion, multi-source analytics, IoT aggregation, and scenarios requiring synchronized processing of multiple independent event streams 
- section9: created app.yaml for tuple implementation - added bindings definition for processor-out-0, processor-out-1; added test pkg/class, FanOutTupleTest
- section9: [BP] implement advanced Fan-Out using Tuple2 pattern without StreamBridge for explicit multiple output streams; Created FanOutProcessor.java demonstrating reactive stream splitting with compile-time type safety using Function<Flux<Message<OrderEvent>>, Tuple2<Flux<DigitalDelivery>, Flux<PhysicalDelivery>>> signature; Implements sophisticated stream sharing via Sinks.Many<OrderEvent> multicast sink feeding two independent output streams: digital deliveries (all orders) and physical deliveries (filtered PHYSICAL orders only); Uses Tuples.of() for explicit output contract, transform() operators for stream processing, and filter() for conditional routing; Added comprehensive section9 package structure mirroring section8 but with pure reactive approach; Configured application-section9.yaml for Tuple2-based topology; Updated application.yaml to section9 profile; Message flow: OrderEvent → shared Sinks.Many → parallel stream processing → Tuple2<Flux<Digital>, Flux<Physical>> → Spring Cloud Stream binding → independent topic routing; Demonstrates enterprise pattern for type-safe multi-output processors, stream analytics systems, and scenarios requiring explicit compile-time output contracts with full reactive stream control
- section8: Added FanOutProcessorMessageBuilder to show implementation of Fanout using MessageBuilder without using StreamBridge and using Tuples
- section8: [BP] implement Fan-Out messaging pattern with content-based routing and advanced functional composition; Created FanOutProcessor.java demonstrating one-to-many message distribution where physical orders trigger fan-out to BOTH digital-delivery-topic AND physical-delivery-topic while digital orders route only to digital-delivery-topic; Implements elegant functional composition using Consumer<OrderEvent> chains with digitalSend.andThen(physicalSend) for fan-out behavior; Added comprehensive section8 package structure: producer.OrderEventProducerConfig (reactive message generation), processor.FanOutProcessor (core routing logic), consumer.DigitalDeliveryConsumer/PhysicalDeliveryConsumer (downstream processors), dto package (OrderEvent/OrderType/DigitalDelivery/PhysicalDelivery records), config.DeliveryChannelProperties (topic constants); Created FanOutTest.java integration test validating fan-out behavior: digital orders → 1 digital delivery, physical orders → 1 digital + 1 physical delivery using Sinks-based test producers and StepVerifier assertions; Configured application-section8.yaml with StreamBridge topology for fan-out scenarios; Updated application.yaml to section8 profile; Message flow: OrderEvent → FanOutProcessor router → digitalSend/fanOut functions → StreamBridge.send() → multiple topics → independent consumers; Demonstrates enterprise fan-out pattern for notification systems, order processing, and multi-channel delivery scenarios
- Accompanying Test for CharFinder below; uses 3 Sinks.Many<String/Char/String> reqSink to emit strings to string-topic, charSink to consume from char-topic, dltSink to consume for dlt-topic
- [BP] wrote a tool, CharFinder, to route a message to Dead-Letter-Topic (DLT) on exception side-effect. Exception side effect is handled via reactive error recovery via .onErrorResume() and Mono.fromRunnable(), which calls handleError() - which writes the message to DLT
- same as previous commit but this time used implicit binding (direct topic)
- section6.*; modified OrderRouter to dynamically route without using StreamBridge, and instead using Message header ("spring.cloud.stream.sendto.destination")
- create + fix section5: resolve OrderRouterTest timeout issues and Spring profile isolation problems; Fixed critical Spring Cloud Function definition bug where commas were incorrectly used instead of semicolons as function separators (orderProcessor,testProducer,testDDConsumer,testPDConsumer → orderProcessor;testProducer;testDDConsumer;testPDConsumer), causing Spring to treat all functions as single invalid function name resulting in no function registration and broken message flow; Added explicit spring.profiles.active=section5 in @TestPropertySource to ensure proper component scanning isolation preventing test interference from other packages; Enhanced AbstractIntegrationTest.java with reactive Kafka testing utilities: added createSender() and toSenderRecord() helper methods for KafkaSender creation and SenderRecord building; Created OrderRouterTest.java with complete integration test using Sinks-based producer/consumer pattern for controlled message emission and StepVerifier for reactive stream testing; Implemented inner @TestConfiguration class with testProducer (Supplier<Flux<OrderEvent>>), testDDConsumer and testPDConsumer (Consumer<Flux<DigitalDelivery/PhysicalDelivery>>) beans for isolated test environment; Test validates complete message routing pipeline: OrderEvent production → orderProcessor consumption → StreamBridge routing → delivery-specific topic distribution → consumer verification; Demonstrates proper Spring Cloud Stream test isolation, function definition syntax, and reactive integration testing patterns; Message flow: testProducer → order-events-topic → orderProcessor → StreamBridge(digital/physical-delivery-out) → delivery topics → test consumers → Sinks capture → StepVerifier assertions
- Content Based Dynamic Routing: Use topic instead of explicit bindings; modified yaml to remove explicit bindings; modified OrderRouter to pass topic to streamBridge.send() instead of binding (explicit)
- modified OrderRouter: removed enum based properties, created utility class  DeliveryChannelProperties instead; created explicit bindings (no cloud function defined) in yaml - digital-delivery-out, physical-delivery-out; modified OrderRouter to use the explicit bindings
- refactor section5: created processor.OrderRouter; implement enum-based channel routing with type safety; Added DeliveryChannel.java enum with OrderType → channel mapping and findByOrderType() lookup method; Modified OrderRouter.java to use enum-based routing eliminating hardcoded channel strings; Added application-section5.yaml with complete function definitions (orderEventProducer;orderProcessor;digitalDeliveryConsumer;physicalDeliveryConsumer) and binding configurations; Implemented type-safe content-based routing pattern using DeliveryChannel.findByOrderType() for compile-time safety; Enhanced sendToChannel() method with generic error handling and consistent logging; Upgraded pom.xml Spring Boot parent 3.5.4 → 3.5.5; Message flow: OrderEvent → DeliveryChannel enum lookup → type-safe StreamBridge.send() → delivery-specific consumers; Demonstrates enterprise pattern replacing magic strings with enums for maintainable, extensible, and type-safe channel management
- section5; created consumer.Digital/PhysicalDeliverConsumer with SC Function physical/digitalDeliveryConsumer():Function<Flux<Message<Physical/DigitalDelivery>>, Mono<Void>>; and printMsgDetails(CustomRecord<Physical/DigitalDelevery>); probably can be refactored to have BaseDeliveryConsumer
- section5: created producer.OrderEventProducerConfig (@Configuration) with Stream Cloud Function orderEventProducer returning Supplier<Flux<Message<OrderEvent>>> (@Bean) (Reactive producer with MessageBuilder pattern), and toMessage() to convert int series in flux to Message<OrderEvent>
- created section5 pkgs: consumer, producer, processor, dto. In section5.dto, created java records to represent messages consumed and produced by processor: dto.OrderEvent (with enum OrderType), dto.DigitalDelivery (DD), dto.PhysicalDelivery (PD). Processor consumes OrderEvent (OE) and produces DD or PD depending on OrderType
- started section5 (new pkg) to demo Content Based Message rounting using StreamBridge.

- refactor section4: upgrade KafkaProducerTest to industry best practices with DTO/Mapper pattern testing; Modified KafkaProducerTestConfiguration.java to use ConcurrentLinkedQueue<CustomRecord<String>> instead of Message<String> for domain-level testing, added MessageConverter::toRecord transformation in testConsumer() bean for Message<String> → CustomRecord<String> mapping; Updated KafkaProducerTest.java to test key-value pairs via testKafkaProducerWithKeyValue() method using CustomRecord assertions for record.message() and record.key() validation, replaced array-based verification with stream().toList() for type safety; Modified application.yaml to use section4 profile; Demonstrates enterprise testing pattern separating Spring Cloud Stream framework concerns (Message<T>) from business domain objects (CustomRecord<T>); Message flow: KafkaProducer → Message<String> with keys → testConsumer → MessageConverter.toRecord() → CustomRecord<String> → ConcurrentLinkedQueue → test verification with payload/key assertions; Enables clean architecture testing with thread-safe capture, proper test isolation, and domain-focused validation while maintaining external configuration separation for better maintainability and reusability across test scenarios
- demonstrate section2: Spring Cloud Stream automatic message wrapping capability; Modified KafkaConsumer.java consumer() bean from Consumer<Flux<String>> to Consumer<Flux<Message<String>>> while keeping KafkaProducer.java unchanged with Supplier<Flux<String>> (simple String payload); Added import org.springframework.messaging.Message; Updated application.yaml to use section2 profile; Demonstrates framework's intelligent auto-wrapping where simple String producers can be consumed as Message<String> objects; Producer sends: "message-4" (String) → Framework auto-wraps → Consumer receives: Message<String> with full metadata (payload, headers, partitionId, offset, acknowledgment, but kafka_receivedMessageKey=null since no key set); Message flow: Flux<String> → SCS auto-wrapping → Message<String> creation → Kafka topic → Message<String> consumer → access to acknowledgment/headers; Shows asymmetric producer-consumer pattern enabling backward compatibility, gradual migration paths, and consumer-side acknowledgment capabilities regardless of producer complexity; Validates that simple payloads work seamlessly with Message-aware consumers for maximum framework flexibility
- enhanced section4: implement manual message acknowledgment with DTO/Mapper pattern; Created CustomRecord.java (DTO) with record CustomRecord<T>(String key, T message, ReceiverOffset acknowledgement) for structured data container; Created MessageConverter.java (mapper) with static toRecord() method extracting KafkaHeaders.RECEIVED_KEY, payload via getPayload(), and KafkaHeaders.ACKNOWLEDGMENT for ReceiverOffset; Modified KafkaConsumer.java to use .map(MessageConverter::toRecord) transformation and printMsgDetails(CustomRecord<String>) with manual rec.acknowledgement().acknowledge() for explicit offset commits; Updated application-section4.yaml with consumer group "some-group" and group.instance.id for static consumer instances; Demonstrates enterprise DTO/Mapper pattern separating Spring framework Message<T> from domain CustomRecord<T>; Message flow: Message<String> → MessageConverter.toRecord() → CustomRecord<String> → process payload/key → manual acknowledgment → offset commit; Enables precise message lifecycle control, prevents duplicate processing on restart, and provides clean separation between messaging framework and business logic
- Added: application-section4.yaml with key serialization configuration; Modified: application.yaml to use section4 profile
- created pkg: kafka.section4 with enhanced messaging using Message<T> wrapper pattern; KafkaProducer modified to produce Supplier<Flux<Message<String>>> using MessageBuilder for structured message creation with payload, Kafka keys (KafkaHeaders.KEY), and custom headers; KafkaConsumer modified to consume Consumer<Flux<Message<String>>> with printMsgDetails() method for extracting payload via msg.getPayload() and headers via msg.getHeaders(); enables rich metadata transport, message partitioning control, and distributed tracing capabilities; message flow: toMessage() → MessageBuilder.withPayload() → setHeader(KafkaHeaders.KEY) → setHeader(custom) → build() → Flux<Message<String>> → Kafka topic → consumer → printMsgDetails() → extract payload/headers for processing
- [BP] refactor: implement industry best practices for KafkaProcessorTest with hybrid approach; Extract KafkaProcessorTestConfiguration as separate class with @TestConfiguration; Implement hybrid pattern: encapsulated Sinks for input, ConcurrentLinkedQueue for output; Add clean public interface with emitMessage() method for controlled message emission; Replace tight coupling with proper separation of concerns using @ContextConfiguration; Add proper test isolation with @BeforeEach cleanup and queue clearing; Enhance error messages with queue contents for improved debugging; Implement message flow: emitMessage() → inputSink → testProducer → processor → testConsumer → verification;  Add testKafkaProcessorWithSingleMessage() for edge case coverage; Fix timing issues ensuring messages flow after Spring context initialization; solve reactive stream timing issue in previous commit
- created KafkaProcessorTest. Coded original version of integration test using sinks; modified AIT to move properties from test impl (KafkaConsumer/ProducerTest) to AIT (logging, offset.reset)
- created scf KafkaProcessor (kafka.section3) to run within a producer-processor-consumer pipeline -> KafkaProducer with producer():Supplier<Flux<String>> emitting periodic messages; KafkaProcessor with processor():Function<Flux<String>,Flux<String>> using flatMap for concurrent message transformation (toUpperCase); KafkaConsumer with consumer():Consumer<Flux<String>> for final consumption; configured application-section3.yaml with function definition (producer;consumer;processor), topic bindings (input-topic → processor → output-topic), and consumer groups (processor-group, consumer-group); updated application.yaml to use section3 profile
- created KafkaProducerTest & KafkaProducerTestConfiguration; Separate test configuration from test logic using @ContextConfiguration;Implement thread-safe message capture with ConcurrentLinkedQueue; Add proper test isolation with @BeforeEach cleanup; Use reactive testing with StepVerifier and Mono.delay(); Remove tight coupling between test and configuration classes; Maintain clean separation of concerns for better maintainability 
- refactored KafkaConsumerTest to extract config into separate KafkaConsumerTestConfiguration class and referenced via @ContextConfiguration
- modified KafkaConsumerTest to add @TestPropertySource, set properties for: scf.definition, scs.bindings.testProducer-out-0.destination, logging.level.root/jayslabs.kafka. test currently defined for KafkaConsumer.consumer()
- src/test/java: create AbstractIntegrationTest (AIT) base test class (uses @EmbeddedKafka, @SpringbootTest, EmbeddedKafkaBroker - @Autowired); created KafkaConsumerTest (extends AIT, @ExtendWith OutputCaptureExtension) with @Test method testKafkaConsumer(CapturedOutput) - uses: Mono, .delay(), .then(), .fromSupplier(), Duration, output::getOut, Duration, .as(), StepVerifier, consumeNextWith(); Inner class @TestConfiguration TestConfig with testProducer():Supplier<Flux<String>>
- prep workspace for integration test: removed dep:scs-test-binder, added SCSAppTest using @EmbeddedKafka
- added code for setting binding properties via @Bean via SenderOptionsCustomizer (deprecated)
- created KafkaProducer with producer():Supplier<Flux<String>>; modified app.yaml to add to scf.definition, scs.bindings (producer-out-0)
- modified app.yaml to set properties based on kafka.binding (function-0, consumer-in-0)
- added code for setting binding properties via @Bean via ReceiverOptionsCustomizer (deprecated)
- modified app.yaml to define binder specific properties: spring.cloud.stream.kafka.binder.<configuration/producer-properties/consumer-properties>, set "group.instance.id" var
- modified app.yaml to setup for active profiles, + application-section2.yaml, + application.yaml, modified sping app to use scanBasePackages appending active profile var ${sec}
- modified KafkaConsumer to add another function bean - function():Function<Flux<String>,Mono<Void>>; modified application.yaml: added binding for function(), and set spring.cloud.function.definition to use function
- pkg: kafka.section2: created KafkaConsumer (@Configuration) with consumer():Consumer<Flux<String>> (@Bean); defined bindings in application.yaml: spring.cloud.stream.bindings
- initial project commit; updated pom reference for spring-cloud-stream-binder-kafka-reactive; readme update

#### proj folder: kafka-setup
- added docker-compose.yaml to setup docker (image: vinsdocker/kafka), volumes r
eferences server.properties; added /data/ to gitignore

#### repo: scs-reactive-kafka-microservices