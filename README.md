# scs-reactive-kafka-microservices
Jay's project/practice repo for Event-driven Microservices using Reactive Kafka and Spring Cloud Stream

#### proj scs-kafka-sandbox (jayaslabs.kafka; SpringBoot 3.5.5, jdk 21; Cloud Stream, Spring for Apache Kafka, Lombok, spring-cloud-stream-binder-kafka-reactive)

- section12: demonstrate multi-topic consumer pattern for single consumer consuming from multiple Kafka topics using comma-separated destination list; Created KafkaConsumer.java with simple Consumer<Flux<String>> consuming from both input-topic1 and input-topic2 via single consumer-in-0 binding; Configured application-section12.yaml with destination: "input-topic1,input-topic2" showing Spring Cloud Stream's built-in multi-topic subscription capability; Created MultiTopicConsumerTest.java integration test with two Sinks.Many<String> (sink1, sink2) simulating producers for producer1 (→ input-topic1) and producer2 (→ input-topic2); Test validates single consumer receives messages from multiple topics in merged order; Key difference from Fan-In pattern (section10): Multi-topic assumes same message type across topics with automatic merge by framework, while Fan-In uses Tuple2<Flux<T1>, Flux<T2>> for different types with explicit Flux.combineLatest() control; Message flow: sink1 → input-topic1 → consumer (merged), sink2 → input-topic2 → consumer (merged); Alternative approach: ReceiverOptionsCustomizer with subscription(List<String> topics) for programmatic multi-topic configuration; Use cases: aggregating logs from multiple services, consuming events from regional topics, merging audit trails; Enterprise pattern for simplified multi-source consumption when topics share same schema and processing logic doesn't require per-topic awareness
- section11: applied default scs (not kafka) properties in yaml, at scs level, for encoding/decoding, removing from binding level
- section11: [BP] successfully resolve polymorphic type deserialization with proper JsonSerializer/JsonDeserializer configuration; Fixed ClassCastException by configuring application-section11.yaml with explicit JsonDeserializer (kafka.binder.consumer-properties.value.deserializer) and critical spring.json.trusted.packages property set to "jayslabs.kafka.section11.dto" for security whitelisting; Configured EncodingDecodingTest with useNativeEncoding=true and JsonSerializer for producer via spring.cloud.stream.kafka.bindings.cmProducer-out-0.producer.configuration.value.serializer; Consumer now successfully deserializes abstract ContactMethod interface into concrete Email and Phone implementations; Demonstrates solution: JsonSerializer preserves concrete type information in JSON payload, JsonDeserializer reconstructs proper types when trusted packages configured; Key insight: spring.json.trusted.packages is mandatory security feature preventing arbitrary class deserialization (prevents deserialization attacks); Message flow: Flux<ContactMethod> (Email/Phone) → JsonSerializer (preserves type) → input-topic → JsonDeserializer (with trusted packages) → Consumer<Flux<ContactMethod>> successfully receives concrete types; Shows enterprise pattern: proper JSON serialization configuration enables polymorphic message handling without Jackson @JsonTypeInfo annotations when using Spring's JsonSerializer/JsonDeserializer; Validates that default Spring Kafka JSON serialization can handle interfaces/abstract types with correct configuration
- section11: [BP] demonstrate encoding/decoding challenges with abstract types and polymorphic message handling in Kafka; Created dto package with ContactMethod interface and concrete implementations (Email record, Phone record) to model polymorphic message types; Created KafkaConsumer.java consuming Consumer<Flux<ContactMethod>> to receive abstract type messages; Created EncodingDecodingTest.java with testEncodingDecodingForAbstractTypes() test and inner TestConfig providing cmProducer bean that emits Flux.just(new Email("test@test.com"), new Phone(14167)); Configured application-section11.yaml with consumer binding cmConsumer-in-0 for input-topic (deserializer config commented out for demonstration); Current implementation throws ClassCastException during deserialization: Spring Cloud Stream's default JSON serialization cannot properly deserialize abstract types without type metadata; Demonstrates critical challenge: when producer sends interface/abstract type (ContactMethod), Kafka stores JSON without concrete type information, causing consumer to fail reconstruction; Shows limitation of default JsonSerializer/JsonDeserializer with polymorphic types - requires Jackson @JsonTypeInfo annotations or custom serializer/deserializer with type hints; Message flow: Flux<ContactMethod> (Email/Phone) → JSON serialization (loses type info) → input-topic → JSON deserialization attempts ContactMethod instantiation → ClassCastException (cannot instantiate interface); Illustrates enterprise pattern challenge for event-driven systems with polymorphic domain models requiring type-safe serialization strategies (Jackson polymorphic type handling, Avro schema evolution, or Protocol Buffers)
- section11: demonstrate native encoding/decoding compatibility and serializer/deserializer configuration in Kafka messaging; Created KafkaConsumer.java consuming Flux<Integer> to show type-specific deserialization; Created EncodingDecodingTest.java using reactive KafkaSender with IntegerSerializer for both key and value to send Integer messages directly to Kafka; Test emits Integer values (1,2,3) to input-topic using createSender() and toSenderRecord() utilities from AbstractIntegrationTest; Demonstrates critical encoding/decoding principle: consumer can only decode messages if producer's serialization format matches consumer's expected type - IntegerSerializer (producer) → IntegerDeserializer (consumer); Configured application-section11.yaml with explicit deserializer properties: kafka.binder.consumer-properties with key.deserializer (IntegerDeserializer) and value.deserializer (IntegerDeserializer) to match producer's IntegerSerializer; Shows importance of serializer/deserializer configuration in Kafka: mismatched encodings (e.g., StringSerializer producer with IntegerDeserializer consumer) cause deserialization failures; Without proper deserializer configuration, Spring Cloud Stream cannot automatically convert binary data to Integer type; Message flow: Flux.range(1,3) → KafkaSender (IntegerSerializer) → input-topic → IntegerDeserializer (configured in YAML) → Consumer<Flux<Integer>>; Demonstrates enterprise pattern for type-safe Kafka messaging, explicit serialization/deserialization configuration, and troubleshooting encoding/decoding issues in distributed systems
- section10: [BP] implement Fan-In pattern for merging multiple input streams into single output; Created HeatIndexCalculator.java demonstrating Tuple2 input signature Function<Tuple2<Flux<Integer>, Flux<Integer>>, Flux<Long>> for consuming from two independent topics (temperature-topic and humidity-topic) and producing heat index calculations; Uses Flux.combineLatest() reactive operator to merge two input streams and apply heat index formula whenever either temperature or humidity changes; Implements NOAA heat index algorithm with 9-coefficient polynomial calculation (c1-c9) combining temperature and humidity values; Created FanInTest.java integration test with three Sinks (tempSink, humidSink, hiSink) validating reactive stream combination and calculation accuracy; Test emits temperature/humidity values and verifies heat index output: (90°F, 55%) → 97, (90°F, 60%) → 100, (94°F, 60%) → 110; Configured bindings for processor-in-0 (temp-topic), processor-in-1 (humid-topic), processor-out-0 (heat-index-topic); Message flow: Tuple2<Flux<temp>, Flux<humid>> → Flux.combineLatest() → heat index calculation → Flux<Long> → heat-index-topic; Demonstrates enterprise Fan-In pattern for sensor data fusion, multi-source analytics, IoT aggregation, and scenarios requiring synchronized processing of multiple independent event streams 
- section9: created app.yaml for tuple implementation - added bindings definition for processor-out-0, processor-out-1; added test pkg/class, FanOutTupleTest
- section9: [BP] implement advanced Fan-Out using Tuple2 pattern without StreamBridge for explicit multiple output streams; Created FanOutProcessor.java demonstrating reactive stream splitting with compile-time type safety using Function<Flux<Message<OrderEvent>>, Tuple2<Flux<DigitalDelivery>, Flux<PhysicalDelivery>>> signature; Implements sophisticated stream sharing via Sinks.Many<OrderEvent> multicast sink feeding two independent output streams: digital deliveries (all orders) and physical deliveries (filtered PHYSICAL orders only); Uses Tuples.of() for explicit output contract, transform() operators for stream processing, and filter() for conditional routing; Added comprehensive section9 package structure mirroring section8 but with pure reactive approach; Configured application-section9.yaml for Tuple2-based topology; Updated application.yaml to section9 profile; Message flow: OrderEvent → shared Sinks.Many → parallel stream processing → Tuple2<Flux<Digital>, Flux<Physical>> → Spring Cloud Stream binding → independent topic routing; Demonstrates enterprise pattern for type-safe multi-output processors, stream analytics systems, and scenarios requiring explicit compile-time output contracts with full reactive stream control
- section8: Added FanOutProcessorMessageBuilder to show implementation of Fanout using MessageBuilder without using StreamBridge and using Tuples
- section8: [BP] implement Fan-Out messaging pattern with content-based routing and advanced functional composition; Created FanOutProcessor.java demonstrating one-to-many message distribution where physical orders trigger fan-out to BOTH digital-delivery-topic AND physical-delivery-topic while digital orders route only to digital-delivery-topic; Implements elegant functional composition using Consumer<OrderEvent> chains with digitalSend.andThen(physicalSend) for fan-out behavior; Added comprehensive section8 package structure: producer.OrderEventProducerConfig (reactive message generation), processor.FanOutProcessor (core routing logic), consumer.DigitalDeliveryConsumer/PhysicalDeliveryConsumer (downstream processors), dto package (OrderEvent/OrderType/DigitalDelivery/PhysicalDelivery records), config.DeliveryChannelProperties (topic constants); Created FanOutTest.java integration test validating fan-out behavior: digital orders → 1 digital delivery, physical orders → 1 digital + 1 physical delivery using Sinks-based test producers and StepVerifier assertions; Configured application-section8.yaml with StreamBridge topology for fan-out scenarios; Updated application.yaml to section8 profile; Message flow: OrderEvent → FanOutProcessor router → digitalSend/fanOut functions → StreamBridge.send() → multiple topics → independent consumers; Demonstrates enterprise fan-out pattern for notification systems, order processing, and multi-channel delivery scenarios
- Accompanying Test for CharFinder below; uses 3 Sinks.Many<String/Char/String> reqSink to emit strings to string-topic, charSink to consume from char-topic, dltSink to consume for dlt-topic
- [BP] wrote a tool, CharFinder, to route a message to Dead-Letter-Topic (DLT) on exception side-effect. Exception side effect is handled via reactive error recovery via .onErrorResume() and Mono.fromRunnable(), which calls handleError() - which writes the message to DLT
- same as previous commit but this time used implicit binding (direct topic)
- section6.*; modified OrderRouter to dynamically route without using StreamBridge, and instead using Message header ("spring.cloud.stream.sendto.destination")
- create + fix section5: resolve OrderRouterTest timeout issues and Spring profile isolation problems; Fixed critical Spring Cloud Function definition bug where commas were incorrectly used instead of semicolons as function separators (orderProcessor,testProducer,testDDConsumer,testPDConsumer → orderProcessor;testProducer;testDDConsumer;testPDConsumer), causing Spring to treat all functions as single invalid function name resulting in no function registration and broken message flow; Added explicit spring.profiles.active=section5 in @TestPropertySource to ensure proper component scanning isolation preventing test interference from other packages; Enhanced AbstractIntegrationTest.java with reactive Kafka testing utilities: added createSender() and toSenderRecord() helper methods for KafkaSender creation and SenderRecord building; Created OrderRouterTest.java with complete integration test using Sinks-based producer/consumer pattern for controlled message emission and StepVerifier for reactive stream testing; Implemented inner @TestConfiguration class with testProducer (Supplier<Flux<OrderEvent>>), testDDConsumer and testPDConsumer (Consumer<Flux<DigitalDelivery/PhysicalDelivery>>) beans for isolated test environment; Test validates complete message routing pipeline: OrderEvent production → orderProcessor consumption → StreamBridge routing → delivery-specific topic distribution → consumer verification; Demonstrates proper Spring Cloud Stream test isolation, function definition syntax, and reactive integration testing patterns; Message flow: testProducer → order-events-topic → orderProcessor → StreamBridge(digital/physical-delivery-out) → delivery topics → test consumers → Sinks capture → StepVerifier assertions
- Content Based Dynamic Routing: Use topic instead of explicit bindings; modified yaml to remove explicit bindings; modified OrderRouter to pass topic to streamBridge.send() instead of binding (explicit)
- modified OrderRouter: removed enum based properties, created utility class  DeliveryChannelProperties instead; created explicit bindings (no cloud function defined) in yaml - digital-delivery-out, physical-delivery-out; modified OrderRouter to use the explicit bindings
- refactor section5: created processor.OrderRouter; implement enum-based channel routing with type safety; Added DeliveryChannel.java enum with OrderType → channel mapping and findByOrderType() lookup method; Modified OrderRouter.java to use enum-based routing eliminating hardcoded channel strings; Added application-section5.yaml with complete function definitions (orderEventProducer;orderProcessor;digitalDeliveryConsumer;physicalDeliveryConsumer) and binding configurations; Implemented type-safe content-based routing pattern using DeliveryChannel.findByOrderType() for compile-time safety; Enhanced sendToChannel() method with generic error handling and consistent logging; Upgraded pom.xml Spring Boot parent 3.5.4 → 3.5.5; Message flow: OrderEvent → DeliveryChannel enum lookup → type-safe StreamBridge.send() → delivery-specific consumers; Demonstrates enterprise pattern replacing magic strings with enums for maintainable, extensible, and type-safe channel management
- section5; created consumer.Digital/PhysicalDeliverConsumer with SC Function physical/digitalDeliveryConsumer():Function<Flux<Message<Physical/DigitalDelivery>>, Mono<Void>>; and printMsgDetails(CustomRecord<Physical/DigitalDelevery>); probably can be refactored to have BaseDeliveryConsumer
- section5: created producer.OrderEventProducerConfig (@Configuration) with Stream Cloud Function orderEventProducer returning Supplier<Flux<Message<OrderEvent>>> (@Bean) (Reactive producer with MessageBuilder pattern), and toMessage() to convert int series in flux to Message<OrderEvent>
- created section5 pkgs: consumer, producer, processor, dto. In section5.dto, created java records to represent messages consumed and produced by processor: dto.OrderEvent (with enum OrderType), dto.DigitalDelivery (DD), dto.PhysicalDelivery (PD). Processor consumes OrderEvent (OE) and produces DD or PD depending on OrderType
- started section5 (new pkg) to demo Content Based Message rounting using StreamBridge.

- refactor section4: upgrade KafkaProducerTest to industry best practices with DTO/Mapper pattern testing; Modified KafkaProducerTestConfiguration.java to use ConcurrentLinkedQueue<CustomRecord<String>> instead of Message<String> for domain-level testing, added MessageConverter::toRecord transformation in testConsumer() bean for Message<String> → CustomRecord<String> mapping; Updated KafkaProducerTest.java to test key-value pairs via testKafkaProducerWithKeyValue() method using CustomRecord assertions for record.message() and record.key() validation, replaced array-based verification with stream().toList() for type safety; Modified application.yaml to use section4 profile; Demonstrates enterprise testing pattern separating Spring Cloud Stream framework concerns (Message<T>) from business domain objects (CustomRecord<T>); Message flow: KafkaProducer → Message<String> with keys → testConsumer → MessageConverter.toRecord() → CustomRecord<String> → ConcurrentLinkedQueue → test verification with payload/key assertions; Enables clean architecture testing with thread-safe capture, proper test isolation, and domain-focused validation while maintaining external configuration separation for better maintainability and reusability across test scenarios
- demonstrate section2: Spring Cloud Stream automatic message wrapping capability; Modified KafkaConsumer.java consumer() bean from Consumer<Flux<String>> to Consumer<Flux<Message<String>>> while keeping KafkaProducer.java unchanged with Supplier<Flux<String>> (simple String payload); Added import org.springframework.messaging.Message; Updated application.yaml to use section2 profile; Demonstrates framework's intelligent auto-wrapping where simple String producers can be consumed as Message<String> objects; Producer sends: "message-4" (String) → Framework auto-wraps → Consumer receives: Message<String> with full metadata (payload, headers, partitionId, offset, acknowledgment, but kafka_receivedMessageKey=null since no key set); Message flow: Flux<String> → SCS auto-wrapping → Message<String> creation → Kafka topic → Message<String> consumer → access to acknowledgment/headers; Shows asymmetric producer-consumer pattern enabling backward compatibility, gradual migration paths, and consumer-side acknowledgment capabilities regardless of producer complexity; Validates that simple payloads work seamlessly with Message-aware consumers for maximum framework flexibility
- enhanced section4: implement manual message acknowledgment with DTO/Mapper pattern; Created CustomRecord.java (DTO) with record CustomRecord<T>(String key, T message, ReceiverOffset acknowledgement) for structured data container; Created MessageConverter.java (mapper) with static toRecord() method extracting KafkaHeaders.RECEIVED_KEY, payload via getPayload(), and KafkaHeaders.ACKNOWLEDGMENT for ReceiverOffset; Modified KafkaConsumer.java to use .map(MessageConverter::toRecord) transformation and printMsgDetails(CustomRecord<String>) with manual rec.acknowledgement().acknowledge() for explicit offset commits; Updated application-section4.yaml with consumer group "some-group" and group.instance.id for static consumer instances; Demonstrates enterprise DTO/Mapper pattern separating Spring framework Message<T> from domain CustomRecord<T>; Message flow: Message<String> → MessageConverter.toRecord() → CustomRecord<String> → process payload/key → manual acknowledgment → offset commit; Enables precise message lifecycle control, prevents duplicate processing on restart, and provides clean separation between messaging framework and business logic
- Added: application-section4.yaml with key serialization configuration; Modified: application.yaml to use section4 profile
- created pkg: kafka.section4 with enhanced messaging using Message<T> wrapper pattern; KafkaProducer modified to produce Supplier<Flux<Message<String>>> using MessageBuilder for structured message creation with payload, Kafka keys (KafkaHeaders.KEY), and custom headers; KafkaConsumer modified to consume Consumer<Flux<Message<String>>> with printMsgDetails() method for extracting payload via msg.getPayload() and headers via msg.getHeaders(); enables rich metadata transport, message partitioning control, and distributed tracing capabilities; message flow: toMessage() → MessageBuilder.withPayload() → setHeader(KafkaHeaders.KEY) → setHeader(custom) → build() → Flux<Message<String>> → Kafka topic → consumer → printMsgDetails() → extract payload/headers for processing
- [BP] refactor: implement industry best practices for KafkaProcessorTest with hybrid approach; Extract KafkaProcessorTestConfiguration as separate class with @TestConfiguration; Implement hybrid pattern: encapsulated Sinks for input, ConcurrentLinkedQueue for output; Add clean public interface with emitMessage() method for controlled message emission; Replace tight coupling with proper separation of concerns using @ContextConfiguration; Add proper test isolation with @BeforeEach cleanup and queue clearing; Enhance error messages with queue contents for improved debugging; Implement message flow: emitMessage() → inputSink → testProducer → processor → testConsumer → verification;  Add testKafkaProcessorWithSingleMessage() for edge case coverage; Fix timing issues ensuring messages flow after Spring context initialization; solve reactive stream timing issue in previous commit
- created KafkaProcessorTest. Coded original version of integration test using sinks; modified AIT to move properties from test impl (KafkaConsumer/ProducerTest) to AIT (logging, offset.reset)
- created scf KafkaProcessor (kafka.section3) to run within a producer-processor-consumer pipeline -> KafkaProducer with producer():Supplier<Flux<String>> emitting periodic messages; KafkaProcessor with processor():Function<Flux<String>,Flux<String>> using flatMap for concurrent message transformation (toUpperCase); KafkaConsumer with consumer():Consumer<Flux<String>> for final consumption; configured application-section3.yaml with function definition (producer;consumer;processor), topic bindings (input-topic → processor → output-topic), and consumer groups (processor-group, consumer-group); updated application.yaml to use section3 profile
- created KafkaProducerTest & KafkaProducerTestConfiguration; Separate test configuration from test logic using @ContextConfiguration;Implement thread-safe message capture with ConcurrentLinkedQueue; Add proper test isolation with @BeforeEach cleanup; Use reactive testing with StepVerifier and Mono.delay(); Remove tight coupling between test and configuration classes; Maintain clean separation of concerns for better maintainability 
- refactored KafkaConsumerTest to extract config into separate KafkaConsumerTestConfiguration class and referenced via @ContextConfiguration
- modified KafkaConsumerTest to add @TestPropertySource, set properties for: scf.definition, scs.bindings.testProducer-out-0.destination, logging.level.root/jayslabs.kafka. test currently defined for KafkaConsumer.consumer()
- src/test/java: create AbstractIntegrationTest (AIT) base test class (uses @EmbeddedKafka, @SpringbootTest, EmbeddedKafkaBroker - @Autowired); created KafkaConsumerTest (extends AIT, @ExtendWith OutputCaptureExtension) with @Test method testKafkaConsumer(CapturedOutput) - uses: Mono, .delay(), .then(), .fromSupplier(), Duration, output::getOut, Duration, .as(), StepVerifier, consumeNextWith(); Inner class @TestConfiguration TestConfig with testProducer():Supplier<Flux<String>>
- prep workspace for integration test: removed dep:scs-test-binder, added SCSAppTest using @EmbeddedKafka
- added code for setting binding properties via @Bean via SenderOptionsCustomizer (deprecated)
- created KafkaProducer with producer():Supplier<Flux<String>>; modified app.yaml to add to scf.definition, scs.bindings (producer-out-0)
- modified app.yaml to set properties based on kafka.binding (function-0, consumer-in-0)
- added code for setting binding properties via @Bean via ReceiverOptionsCustomizer (deprecated)
- modified app.yaml to define binder specific properties: spring.cloud.stream.kafka.binder.<configuration/producer-properties/consumer-properties>, set "group.instance.id" var
- modified app.yaml to setup for active profiles, + application-section2.yaml, + application.yaml, modified sping app to use scanBasePackages appending active profile var ${sec}
- modified KafkaConsumer to add another function bean - function():Function<Flux<String>,Mono<Void>>; modified application.yaml: added binding for function(), and set spring.cloud.function.definition to use function
- pkg: kafka.section2: created KafkaConsumer (@Configuration) with consumer():Consumer<Flux<String>> (@Bean); defined bindings in application.yaml: spring.cloud.stream.bindings
- initial project commit; updated pom reference for spring-cloud-stream-binder-kafka-reactive; readme update

#### proj folder: kafka-setup
- added docker-compose.yaml to setup docker (image: vinsdocker/kafka), volumes r
eferences server.properties; added /data/ to gitignore

#### repo: scs-reactive-kafka-microservices